{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from typing import List\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://www.nogizaka46.com/s/n46/diary/detail/'\n",
    "NEWEST_URL = BASE_URL + '102848?ima=3554&cd=MEMBER'\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_member_page(member_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the HTML of a given member's page.\n",
    "\n",
    "    Args:\n",
    "        member_url (str): URL of the member's blog page.\n",
    "\n",
    "    Returns:\n",
    "        str: HTML content of the page.\n",
    "    \"\"\"\n",
    "    response = requests.get(member_url, headers=HEADERS)\n",
    "    response.raise_for_status()\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_title(html_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse the images from the member's blog page HTML content.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of image URLs.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title_tag = soup.find('meta', property='og:title')\n",
    "    return title_tag['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(html_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse the images from the member's blog page HTML content.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of image URLs.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    date_tag = soup.find('p', class_='bd--hd__date a--tx js-tdi')\n",
    "    return date_tag.text.strip().replace(':', '：').replace('.', '-').replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_images(html_content: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse the images from the member's blog page HTML content.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of image URLs.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    images = []\n",
    "    for img_tag in soup.find_all('img'):\n",
    "        img_src = img_tag.get('src')\n",
    "        img_src = 'https://www.nogizaka46.com' + img_src\n",
    "        if img_src and img_src.startswith('http'):\n",
    "            images.append(img_src)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(image_urls: List[str], save_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the list of image URLs to a local directory named after the member.\n",
    "\n",
    "    Args:\n",
    "        image_urls (List[str]): List of image URLs.\n",
    "        member_name (str): Name of the member.\n",
    "    \"\"\"\n",
    "    dir_name = f\"nogizaka/{save_dir}\"\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    for image_url in image_urls:\n",
    "        image_data = requests.get(image_url).content\n",
    "        image_name = os.path.join(dir_name, os.path.basename(image_url))\n",
    "        with open(image_name, 'wb') as image_file:\n",
    "            image_file.write(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_texts(html_content: str, save_dir: str) -> str:\n",
    "    dir_name = f\"nogizaka/{save_dir}\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    raw_text = soup.find('div', class_='bd--edit')\n",
    "    divs = raw_text.find_all('div')\n",
    "    text_name = os.path.join(dir_name, 'blog.txt')\n",
    "    text = ''\n",
    "    for div in divs:\n",
    "        text = text + div.get_text() + '\\n'\n",
    "    with open(text_name, 'w') as text_file:\n",
    "        text_file.write(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_previous_url(html_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse the images from the member's blog page HTML content.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of image URLs.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    url_tag = soup.find('a', class_='bd--hn__a hv--op')\n",
    "    previous_or_next = url_tag.find('p', class_='bd--hn__tx f--head').text.strip()\n",
    "    url = url_tag['href'] if previous_or_next == '前の記事' else None\n",
    "    if not url is None:\n",
    "        url = 'https://www.nogizaka46.com' + url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the crawling and scraping process.\n",
    "    \"\"\"\n",
    "    # Implement the process to get URLs of each member's blog.\n",
    "    newest_url = NEWEST_URL  # Example actual URL\n",
    "    html = fetch_member_page(newest_url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    name_tag = soup.find('p', class_='bd--prof__name f--head')\n",
    "    member_name = name_tag.text.strip().replace(' ', '')\n",
    "    url = newest_url\n",
    "    count = 1\n",
    "    \n",
    "    while True:\n",
    "        html = fetch_member_page(url)\n",
    "        title = parse_title(html)\n",
    "        date = parse_date(html)\n",
    "        images = parse_images(html)\n",
    "        save_dir = member_name + '/' + '【' + date + '】' + title\n",
    "        save_images(images, save_dir)\n",
    "        save_texts(html, save_dir)\n",
    "        print(f'取り込み数: {count} {url}', end='\\r', flush=True)\n",
    "        url = parse_previous_url(html)\n",
    "        if url is None:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "            time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取り込み数: 112 https://www.nogizaka46.com/s/n46/diary/detail/43327?ima=2754&cd=MEMBER\r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
